{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b82be6f",
   "metadata": {},
   "source": [
    "# <center>**`Project Details`**</center>\n",
    "\n",
    "#### **Purpose**:\n",
    "This project goal is to build, evaluate, and iteratively improve a fine-tuned LLM that generates domain name suggestions for businesses. \n",
    "\n",
    "The focus will be not only on generating names but also on systematic evaluation, edge case discovery, and improvement cycles.\n",
    "\n",
    "\n",
    "#### **Deliverables**:\n",
    "\n",
    "1. Code & Setup\n",
    "\n",
    "    - Git repo with reproducible code + setup instructions (Python).\n",
    "    - Jupyter notebook with all experiments.\n",
    "\n",
    "2. Experimentation & Tracking\n",
    "\n",
    "    - Model versioning and checkpoint management.\n",
    "\n",
    "    - Evaluation framework reusable across iterations.\n",
    "\n",
    "3. Evaluation & Report\n",
    "\n",
    "    - Technical report summarizing methodology, dataset, evaluation, improvements, and recommendations.\n",
    "\n",
    "4. (Optional) Deployment\n",
    "\n",
    "    - Simple API endpoint for inference (JSON in/out format).\n",
    "\n",
    "\n",
    "#### **Required Components**:\n",
    "\n",
    "1. Synthetic Dataset Creation\n",
    "\n",
    "    - Build initial dataset of business descriptions → domain names.\n",
    "    - Ensure diversity in business types and complexity.\n",
    "    - Document dataset generation method.\n",
    "\n",
    "2. Model Development & Iteration\n",
    "\n",
    "    - Start with a base open-source LLM (e.g., LLaMA, Mistral).\n",
    "\n",
    "    - Improve through:\n",
    "\n",
    "        - Dataset augmentation.\n",
    "\n",
    "        - Fine-tuning strategies (LoRA, full fine-tune, QLoRA, etc.).\n",
    "\n",
    "        - Hyperparameter tuning.\n",
    "\n",
    "    - Save and version checkpoints.\n",
    "\n",
    "3. LLM-as-a-Judge Evaluation\n",
    "\n",
    "    - Automated evaluation pipeline where an LLM (e.g., GPT-4, Claude, or fine-tuned model) scores domain quality.\n",
    "\n",
    "    - Define a systematic scoring rubric (e.g., relevance, creativity, readability, safety).\n",
    "\n",
    "4. Edge Case Discovery & Analysis\n",
    "\n",
    "    - Systematically find and analyze model failure modes.\n",
    "\n",
    "    - Categorize failures, measure frequency, and propose fixes.\n",
    "\n",
    "    - Show measurable improvements over iterations.\n",
    "\n",
    "5. Safety Guardrails\n",
    "\n",
    "    - Ensure the model blocks harmful/inappropriate requests (e.g., adult, offensive).\n",
    "\n",
    "    - Document and test safety filter.\n",
    "\n",
    "\n",
    "#### **Model & Tech Requirements**:\n",
    "\n",
    " - **Generator**: Must use an open-source LLM (LLaMA, Mistral, etc.).\n",
    "\n",
    " - **Evaluator (judge)**: Can use either third-party APIs (GPT-4, Claude) or a fine-tuned open-source model.\n",
    "\n",
    " - All code must be reproducible.\n",
    "\n",
    "\n",
    "#### **Technical Report Structure**:\n",
    "\n",
    "1. Methodology & Initial Results.\n",
    "\n",
    "2. Edge Case Analysis (taxonomy, frequency).\n",
    "\n",
    "3. Iterative Improvements (strategies + before/after metrics).\n",
    "\n",
    "4. Model Comparison & Recommendations (production readiness, future improvements).\n",
    "\n",
    "\n",
    "#### **Optional API**:\n",
    "\n",
    " - Input: JSON with business_description.\n",
    "\n",
    " - Output: JSON with suggested domains + confidence scores.\n",
    "\n",
    " - Safety: Block inappropriate inputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786816a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5260bc82",
   "metadata": {},
   "source": [
    "## <center>**`Implementation`**</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0d9e0",
   "metadata": {},
   "source": [
    "#### Check gpus availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99b5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gpus availability\n",
    "import torch\n",
    "\n",
    "print(f\"GPU Available: {torch.cuda.is_available()}\")  \n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6358de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6b13e1c",
   "metadata": {},
   "source": [
    "# 1- Baseline performances\n",
    "\n",
    "Let's first establish the basline performances. It means:\n",
    "\n",
    "- Create synthetic dataset: It will be saved for next iterations \n",
    "- Load a foundation model + tokenizer\n",
    "- Generate domain names using the foundation model\n",
    "- Score the foundation model domain names suggestions\n",
    "- Analyze foundation model performances: this is to guide us on ideas for improvements\n",
    "\n",
    "As some parts of above steps will be reused in next iterations, it is a good idea to set them as reusable function/module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17884374",
   "metadata": {},
   "source": [
    "## Helper components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6302a6d",
   "metadata": {},
   "source": [
    "### Templates & Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709a9ef1",
   "metadata": {},
   "source": [
    "#### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60961f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/templates/constants.py\n",
    "# src/templates/constants.py\n",
    "\n",
    "JSON_ARRAY_REGEX = r\"\\[.*?\\]\"\n",
    "\n",
    "\n",
    "INDUSTRIES = [\n",
    "    \"organic coffee shop\",\"AI consulting\",\"children toys\",\"cybersecurity SaaS\",\n",
    "    \"yoga studio\",\"bakery\",\"fintech lending\",\"eco cosmetics\",\"pet grooming\",\n",
    "    \"travel planner\",\"real estate agency\",\"data labeling service\",\"mobile game studio\",\n",
    "    \"local bike repair\",\"language school\",\"artisan bakery\",\n",
    "]\n",
    "\n",
    "STYLES = [\"premium\",\"playful\",\"minimalist\",\"techy\",\"eco\",\"luxury\"]\n",
    "\n",
    "TLDS = [\".com\",\".io\",\".co\",\".ai\",\".app\",\".dev\",\".org\",\".net\"]\n",
    "\n",
    "UNSAFE_THEMES = [\n",
    "    \"adult content\", \"weapons marketplace\", \"illegal drugs\", \"hate group\",\n",
    "    \"deepfake service\", \"fake IDs\", \"terror propaganda\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a350ea67",
   "metadata": {},
   "source": [
    "#### Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01829d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/templates/prompts.py\n",
    "# src/templates/prompts.py\n",
    "\n",
    "\n",
    "GEN_PROMPT_TEMP = (\n",
    "    \"You are a creative assistant suggesting domain names.\\n\\n\"\n",
    "        \"Business: {desc}\\n\\n\"\n",
    "        \"Rules:\\n\"\n",
    "        \"- lowercase only\\n\"\n",
    "        \"- 3-10 letters before the TLD\\n\"\n",
    "        \"- no numbers, no leading/trailing hyphens, no profanity\\n\"\n",
    "        \"- prefer .com, .io, .org, .net\\n\\n\"\n",
    "        \"Return exactly {n} domain names as a JSON array of strings.\\n\"\n",
    "        'Example: [\"brandly.com\", \"neocafe.io\", \"greenbrew.org\"]')\n",
    "\n",
    "SFT_PROMPT_TEMP = (\n",
    "    \"You are a helpful assistant that suggests short, brandable domain names.\\n\"\n",
    "    \"Rules: lowercase, avoid numbers, avoid leading/trailing hyphens, avoid profanity, 3-10 letters before TLD.\\n\"\n",
    "    \"Return ONLY a JSON array of domain strings.\\n\\n\"\n",
    "    \"Business: {desc}\"\n",
    ")\n",
    "\n",
    "JUDGE_SYSTEM_PROMPT = (\n",
    "    \"You are a strict, consistent judge for domain name suggestions. \"\n",
    "    \"Return only valid JSON.\\n\\n\"\n",
    "    \"Scores (0.0–1.0): relevance, memorability, readability, safety.\\n\"\n",
    "    \"Compute 'overall' as weighted average using provided weights.\"\n",
    ")\n",
    "JUDGE_USER_PROMPT_TEMP = (\n",
    "    \"Evaluate domain suggestions for the business.\\n\\n\"\n",
    "    \"Business:\\n{business}\\n\\n\"\n",
    "    \"Suggestions (JSON array of strings):\\n{suggestions}\\n\\n\"\n",
    "    \"Weights (JSON):\\n{weights}\\n\\n\"\n",
    "    \"Return a JSON array like:\\n\"\n",
    "    '[{{\"domain\":\"...\", \"relevance\":0.8, \"memorability\":0.7, \"readability\":0.9, \"safety\":1.0, \"overall\":0.84}}]'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe67425",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/cfg.py\n",
    "# src/cfg.py\n",
    "\n",
    "import yaml\n",
    "\n",
    "def load_config(path: str = \"config.yaml\") -> dict:\n",
    "    \"\"\"Load YAML config file\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8270cb",
   "metadata": {},
   "source": [
    "### JSON extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/utils_json.py\n",
    "# src/utils_json.py\n",
    "\n",
    "import json, re\n",
    "from typing import List\n",
    "from templates.constants import JSON_ARRAY_REGEX\n",
    "\n",
    "def extract_json_array(text: str) -> List[str]:\n",
    "    \"\"\"Utilities for robust JSON array extraction from model text outputs.\n",
    "    Return last JSON array from text as list[str]; [] on failure.\"\"\"\n",
    "    matches = re.findall(JSON_ARRAY_REGEX, text, flags=re.S)\n",
    "    if not matches:\n",
    "        return []\n",
    "    for candidate in reversed(matches):\n",
    "        norm = candidate.strip()\n",
    "        # Soft fix for single quotes\n",
    "        if \"'\" in norm and '\"' not in norm:\n",
    "            norm = norm.replace(\"'\", '\"')\n",
    "        try:\n",
    "            parsed = json.loads(norm)\n",
    "            if isinstance(parsed, list):\n",
    "                out = []\n",
    "                for x in parsed:\n",
    "                    if isinstance(x, (str, int, float)):\n",
    "                        out.append(str(x).strip().lower())\n",
    "                # dedup preserve order\n",
    "                seen, dedup = set(), []\n",
    "                for d in out:\n",
    "                    if d not in seen:\n",
    "                        seen.add(d)\n",
    "                        dedup.append(d)\n",
    "                return dedup\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad58bdc5",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a477c",
   "metadata": {},
   "source": [
    "#### Raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ae293",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/data_synth.py\n",
    "# src/data_synth.py\n",
    "\n",
    "import json, random, pathlib, re\n",
    "from templates.constants import INDUSTRIES, STYLES, TLDS, UNSAFE_THEMES\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\",\"-\",s.lower())\n",
    "    s = re.sub(r\"[^a-z0-9-]\",\"\",s)\n",
    "    s = re.sub(r\"-{2,}\",\"-\",s).strip(\"-\")\n",
    "    return s[:12]\n",
    "\n",
    "def synth_targets(desc: str, n: int = 5):\n",
    "    parts = re.findall(r\"[a-z0-9]+\", desc.lower())\n",
    "    core = slugify((parts[0] if parts else \"brand\") + \"-\" + (parts[-1] if parts else \"shop\"))\n",
    "    palette = {\n",
    "        \"premium\":[core,core+\"prime\",core+\"elite\",\"haus\"+core],\n",
    "        \"playful\":[core+\"ly\",\"go\"+core,core+\"buddy\",core+\"fun\"],\n",
    "        \"minimalist\":[core,core[:8],core.replace(\"-\",\"\")],\n",
    "        \"techy\":[core+\"tech\",\"get\"+core,\"try\"+core,core+\"hub\"],\n",
    "        \"eco\":[\"green\"+core,\"eco\"+core,core+\"earth\"],\n",
    "        \"luxury\":[core+\"lux\",core+\"atelier\",core+\"maison\"],\n",
    "    }\n",
    "    style = random.choice(STYLES)\n",
    "    outs = []\n",
    "    for root in palette[style]:\n",
    "        root = re.sub(r\"-{2,}\",\"-\",root).strip(\"-\")\n",
    "        outs.append(root + random.choice(TLDS))\n",
    "    # dedup\n",
    "    seen, dedup = set(), []\n",
    "    for d in outs:\n",
    "        if d not in seen:\n",
    "            seen.add(d); dedup.append(d)\n",
    "    return dedup[:n]\n",
    "\n",
    "def main():\n",
    "    \"\"\"Create a synthetic dataset for domain-name suggestions.\"\"\"\n",
    "    random.seed(42)\n",
    "    out = pathlib.Path(\"data/raw/synth.jsonl\")\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    rows = []\n",
    "    for _ in range(3200):\n",
    "        ind = random.choice(INDUSTRIES)\n",
    "        style = random.choice(STYLES)\n",
    "        geo = random.choice([\"in downtown area\",\"for freelancers\",\"for families\",\"subscription-based\"])\n",
    "        rows.append({\"business_desc\": f\"{ind} {geo} ({style} vibe)\", \"targets\": synth_targets(f\"{ind} {geo}\"), \"safety\":\"safe\"})\n",
    "    for t in UNSAFE_THEMES:\n",
    "        rows.append({\"business_desc\": f\"{t} website\", \"targets\": [], \"safety\":\"unsafe\"})\n",
    "    with out.open(\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in rows: \n",
    "            f.write(json.dumps(r,ensure_ascii=False)+\"\\n\")\n",
    "    print(f\"[data_synth] Wrote {len(rows)} -> {out}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d68ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/data_synth.py\n",
    "# src/data_synth.py\n",
    "\n",
    "import os, re, json, time, random, pathlib\n",
    "from typing import List, Dict, Optional\n",
    "from dotenv import load_dotenv\n",
    "from cfg import load_config\n",
    "from templates.constants import INDUSTRIES, STYLES, TLDS, UNSAFE_THEMES, JSON_ARRAY_REGEX\n",
    "from templates.prompts import GEN_PROMPT_TEMP\n",
    "\n",
    "load_dotenv(\"/workspace/.env\")\n",
    "\n",
    "\n",
    "# Utilities\n",
    "def _slugify(label: str) -> str:\n",
    "    label = re.sub(r\"\\s+\", \"-\", label.lower())\n",
    "    label = re.sub(r\"[^a-z0-9-]\", \"\", label)\n",
    "    label = re.sub(r\"-{2,}\", \"-\", label).strip(\"-\")\n",
    "    return label[:12]\n",
    "\n",
    "def _clean_domain(d: str) -> str:\n",
    "    d = d.strip().lower()\n",
    "    d = re.sub(r\"[^a-z0-9\\.-]\", \"\", d)\n",
    "    d = re.sub(r\"-{2,}\", \"-\", d)\n",
    "    return d.strip(\".-\")\n",
    "\n",
    "def _dedup_keep_order(arr: List[str]) -> List[str]:\n",
    "    seen, out = set(), []\n",
    "    for x in arr:\n",
    "        if not x: continue\n",
    "        if x not in seen:\n",
    "            seen.add(x); out.append(x)\n",
    "    return out\n",
    "\n",
    "# Rule-based generator\n",
    "def synth_targets_rule(desc: str, n: int) -> List[str]:\n",
    "    parts = re.findall(r\"[a-z0-9]+\", desc.lower())\n",
    "    core = _slugify((parts[0] if parts else \"brand\") + \"-\" + (parts[-1] if parts else \"shop\"))\n",
    "    palette = {\n",
    "        \"premium\":   [core, core+\"prime\", core+\"elite\", \"haus\"+core],\n",
    "        \"playful\":   [core+\"ly\", \"go\"+core, core+\"buddy\", core+\"fun\"],\n",
    "        \"minimalist\":[core, core[:8], core.replace(\"-\", \"\")],\n",
    "        \"techy\":     [core+\"tech\", \"get\"+core, \"try\"+core, core+\"hub\"],\n",
    "        \"eco\":       [\"green\"+core, \"eco\"+core, core+\"earth\"],\n",
    "        \"luxury\":    [core+\"lux\", core+\"atelier\", core+\"maison\"],\n",
    "    }\n",
    "    style = random.choice(STYLES)\n",
    "    outs = []\n",
    "    for root in palette[style]:\n",
    "        root = re.sub(r\"-{2,}\", \"-\", root).strip(\"-\")\n",
    "        outs.append(root + random.choice(TLDS))\n",
    "    outs = [_clean_domain(x) for x in outs]\n",
    "    return _dedup_keep_order(outs)[:n]\n",
    "\n",
    "# LLM-based generator\n",
    "_openai_client = None\n",
    "def _get_openai_client():\n",
    "    global _openai_client\n",
    "    if _openai_client is None:\n",
    "        from openai import OpenAI\n",
    "        key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "        if not key:\n",
    "            raise EnvironmentError(\"OPENAI_API_KEY is not set but LLM backend requested.\")\n",
    "        _openai_client = OpenAI(api_key=key)\n",
    "    return _openai_client\n",
    "\n",
    "def _llm_prompt(desc: str, n: int) -> str:    \n",
    "    return GEN_PROMPT_TEMP.format(desc=desc, n=n)\n",
    "\n",
    "def _parse_llm_domains(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Accept either:\n",
    "      - A raw JSON array ([\"a.com\", ...])\n",
    "      - An object like {\"domains\": [\"a.com\", ...]}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = json.loads(text)\n",
    "    except Exception:\n",
    "        # Fallback: Use regex to find JSON array\n",
    "        match = re.findall(JSON_ARRAY_REGEX, text, flags=re.S)\n",
    "        if not match:\n",
    "            return []\n",
    "        try:\n",
    "            data = json.loads(match[-1])\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(data, list):\n",
    "        arr = data\n",
    "    elif isinstance(data, dict):\n",
    "        arr = data.get(\"domains\", [])\n",
    "    else:\n",
    "        arr = []\n",
    "    out = []\n",
    "    for x in arr:\n",
    "        if isinstance(x, (str, int, float)):\n",
    "            out.append(_clean_domain(str(x)))\n",
    "    return _dedup_keep_order(out)\n",
    "\n",
    "def synth_targets_llm(desc: str, n: int, model: str, temperature: float, top_p: float,\n",
    "                      max_retries: int, sleep_sec: float) -> List[str]:\n",
    "    client = _get_openai_client()\n",
    "    prompt = _llm_prompt(desc, n)\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            resp = client.chat.completions.create(\n",
    "                model=model,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                max_tokens=200,\n",
    "                response_format={\"type\": \"json_object\"},  # encourages strict JSON\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            content = (resp.choices[0].message.content or \"\").strip()\n",
    "            domains = _parse_llm_domains(content)\n",
    "            return domains[:n]\n",
    "        except Exception as e:\n",
    "            if attempt >= max_retries:\n",
    "                # Too many attempts\n",
    "                return []\n",
    "            time.sleep(sleep_sec * attempt)\n",
    "    return []\n",
    "\n",
    "# Synthesis\n",
    "def _make_safe_record(desc: str, targets: List[str]) -> Dict:\n",
    "    targets = [_clean_domain(x) for x in targets][:5]\n",
    "    return {\"business_desc\": desc, \"targets\": targets, \"safety\": \"safe\"}\n",
    "\n",
    "def _make_unsafe_records(n: int) -> List[Dict]:\n",
    "    rows = []\n",
    "    for i in range(n):\n",
    "        theme = random.choice(UNSAFE_THEMES)\n",
    "        rows.append({\n",
    "            \"business_desc\": f\"{theme} website\",\n",
    "            \"targets\": [],\n",
    "            \"safety\": \"unsafe\",\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def main():\n",
    "    cfg = load_config()\n",
    "    s = cfg[\"dataset\"][\"raw\"]\n",
    "    random.seed(cfg.get(\"seed\", 42))\n",
    "\n",
    "    out_path = pathlib.Path(s[\"path\"])\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "\n",
    "    # Mix generation method if hybrid\n",
    "    def choose_backend() -> str:\n",
    "        if s[\"backend\"] == \"hybrid\":\n",
    "            return \"llm\" if random.random() < float(s[\"hybrid_ratio\"]) else \"rule\"\n",
    "        return s[\"backend\"]\n",
    "\n",
    "    N = int(s.get(\"N\", 3200))   # number of safe rows to generate\n",
    "    for _ in range(N):\n",
    "        ind = random.choice(INDUSTRIES)\n",
    "        style = random.choice(STYLES)\n",
    "        geo = random.choice([\"in downtown area\", \"for freelancers\", \"for families\", \"subscription-based\"])\n",
    "        desc = f\"{ind} {geo} ({style} vibe)\"\n",
    "\n",
    "        backend = choose_backend()\n",
    "        if backend == \"llm\":\n",
    "            targets = synth_targets_llm(\n",
    "                desc, s[\"n_per_desc\"], \n",
    "                s[\"llm_model\"], \n",
    "                s[\"temperature\"], \n",
    "                s[\"top_p\"], \n",
    "                s[\"max_retries\"], \n",
    "                s[\"sleep_sec\"]\n",
    "            )\n",
    "            # fallback to rule if LLM failed\n",
    "            if not targets:\n",
    "                targets = synth_targets_rule(desc, s[\"n_per_desc\"])\n",
    "        else:\n",
    "            targets = synth_targets_rule(desc, s[\"n_per_desc\"])\n",
    "\n",
    "        rows.append(_make_safe_record(desc, targets))\n",
    "\n",
    "    # Add unsafe negatives\n",
    "    unsafe_ratio = float(s.get(\"unsafe_multiplier\", 0.1))\n",
    "    n_unsafe = max(1, int(N * unsafe_ratio)) if unsafe_ratio > 0 else 0\n",
    "    rows.extend(_make_unsafe_records(n_unsafe))\n",
    "\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[data_synth] backend={s['backend']} | rows={len(rows)} -> {out_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044a07b",
   "metadata": {},
   "source": [
    "#### Prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/data_prep.py\n",
    "# src/data_prep.py\n",
    "\n",
    "import json, pathlib, random, re\n",
    "from templates.prompts import SFT_PROMPT_TEMP\n",
    "\n",
    "\n",
    "def clean(d: str) -> str:\n",
    "    d = d.strip().lower()\n",
    "    d = re.sub(r\"[^a-z0-9\\.-]\",\"\", d)\n",
    "    d = re.sub(r\"-{2,}\",\"-\", d)\n",
    "    return d.strip(\".-\")\n",
    "\n",
    "def fmt(r: dict) -> dict:\n",
    "    prompt = SFT_PROMPT_TEMP.format(desc=r[\"business_desc\"])\n",
    "    if r[\"safety\"]==\"unsafe\":\n",
    "        resp = \"[]\"\n",
    "    else:\n",
    "        tgts = []\n",
    "        seen=set()\n",
    "        for x in r.get(\"targets\",[]):\n",
    "            x = clean(x)\n",
    "            if x and x not in seen:\n",
    "                seen.add(x); tgts.append(x)\n",
    "        resp = json.dumps(tgts[:5], ensure_ascii=False)\n",
    "    return {\"prompt\": prompt, \"response\": resp, \"safety\": r[\"safety\"]}\n",
    "\n",
    "def main():\n",
    "    \"\"\"Clean/split raw into SFT JSONL (train/val).\"\"\"\n",
    "    raw = pathlib.Path(\"data/raw/synth.jsonl\")\n",
    "    rows = [json.loads(l) for l in raw.read_text(encoding=\"utf-8\").splitlines()]\n",
    "    random.seed(42)\n",
    "    random.shuffle(rows)\n",
    "    n=len(rows); ntr=int(0.8*n)\n",
    "    train, val = rows[:ntr], rows[ntr:]\n",
    "    out_tr = pathlib.Path(\"data/processed/train.jsonl\")\n",
    "    out_va = pathlib.Path(\"data/processed/val.jsonl\")\n",
    "    out_tr.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_tr.open(\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in train: \n",
    "            f.write(json.dumps(fmt(r),ensure_ascii=False)+\"\\n\")\n",
    "    with out_va.open(\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in val: \n",
    "            f.write(json.dumps(fmt(r),ensure_ascii=False)+\"\\n\")\n",
    "    print(f\"[data_prep] Train {len(train)} | Val {len(val)}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/data_prep.py\n",
    "# src/data_prep.py\n",
    "\n",
    "import json, pathlib, random, re\n",
    "from typing import Dict, List\n",
    "from cfg import load_config\n",
    "from templates.prompts import SFT_PROMPT_TEMP\n",
    "\n",
    "\n",
    "# Cleaning domain\n",
    "def _clean_domain(d: str) -> str:\n",
    "    d = d.strip().lower()\n",
    "    d = re.sub(r\"[^a-z0-9\\.-]\", \"\", d)\n",
    "    d = re.sub(r\"-{2,}\", \"-\", d)\n",
    "    return d.strip(\".-\")\n",
    "\n",
    "def _format_record(r: Dict) -> Dict:\n",
    "    prompt = SFT_PROMPT_TEMP.format(desc=r[\"business_desc\"])\n",
    "    if r[\"safety\"] == \"unsafe\":\n",
    "        response = \"[]\"\n",
    "    else:\n",
    "        tgts = []\n",
    "        seen = set()\n",
    "        for x in r.get(\"targets\", []) or []:\n",
    "            x = _clean_domain(str(x))\n",
    "            if x and x not in seen:\n",
    "                seen.add(x); tgts.append(x)\n",
    "        response = json.dumps(tgts[:5], ensure_ascii=False)\n",
    "    return {\"prompt\": prompt, \"response\": response, \"safety\": r[\"safety\"]}\n",
    "\n",
    "# Stratified split\n",
    "def _stratified_split(safe_rows: List[Dict], unsafe_rows: List[Dict], train_ratio: float, seed: int):\n",
    "    random.Random(seed).shuffle(safe_rows)\n",
    "    random.Random(seed + 1).shuffle(unsafe_rows)\n",
    "\n",
    "    n_safe = len(safe_rows)\n",
    "    n_unsafe = len(unsafe_rows)\n",
    "    n_safe_tr = int(round(n_safe * train_ratio))\n",
    "    n_unsafe_tr = int(round(n_unsafe * train_ratio))\n",
    "\n",
    "    train = safe_rows[:n_safe_tr] + unsafe_rows[:n_unsafe_tr]\n",
    "    val   = safe_rows[n_safe_tr:] + unsafe_rows[n_unsafe_tr:]\n",
    "\n",
    "    # Stable shuffle within each split for better mixing\n",
    "    random.Random(seed + 2).shuffle(train)\n",
    "    random.Random(seed + 3).shuffle(val)\n",
    "\n",
    "    return train, val, (n_safe, n_unsafe, n_safe_tr, n_unsafe_tr)\n",
    "\n",
    "def main():\n",
    "    cfg = load_config()\n",
    "    seed = int(cfg[\"seed\"])\n",
    "    train_ratio = float(cfg[\"dataset\"][\"processed\"][\"train_ratio\"])\n",
    "\n",
    "    raw_path = pathlib.Path(cfg[\"dataset\"][\"raw\"][\"path\"])\n",
    "    rows = [json.loads(l) for l in raw_path.read_text(encoding=\"utf-8\").splitlines()]\n",
    "\n",
    "    # Stratify by safety\n",
    "    safe_rows = [r for r in rows if r.get(\"safety\") == \"safe\"]\n",
    "    unsafe_rows = [r for r in rows if r.get(\"safety\") == \"unsafe\"]\n",
    "\n",
    "    train_raw, val_raw, stats = _stratified_split(safe_rows, unsafe_rows, train_ratio, seed)\n",
    "    n_safe, n_unsafe, n_safe_tr, n_unsafe_tr = stats\n",
    "\n",
    "    # Format to SFT schema\n",
    "    train_fmt = [_format_record(r) for r in train_raw]\n",
    "    val_fmt   = [_format_record(r) for r in val_raw]\n",
    "\n",
    "    # Write\n",
    "    out_tr = pathlib.Path(cfg[\"dataset\"][\"processed\"][\"train_path\"])\n",
    "    out_va = pathlib.Path(cfg[\"dataset\"][\"processed\"][\"val_path\"])\n",
    "    out_tr.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with out_tr.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in train_fmt:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "    with out_va.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in val_fmt:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # Ratios report\n",
    "    def _ratio(a, b): return round(a / b, 4) if b else 0.0\n",
    "    tr_safe = sum(1 for r in train_raw if r[\"safety\"] == \"safe\")\n",
    "    tr_unsafe = len(train_raw) - tr_safe\n",
    "    va_safe = sum(1 for r in val_raw if r[\"safety\"] == \"safe\")\n",
    "    va_unsafe = len(val_raw) - va_safe\n",
    "\n",
    "    print(\n",
    "        \"[data_prep] Stratified split completed\\n\"\n",
    "        f\"  Total: {len(rows)}  | Safe: {n_safe}  | Unsafe: {n_unsafe}  \"\n",
    "        f\"(unsafe ratio = {_ratio(n_unsafe, len(rows))})\\n\"\n",
    "        f\"  Train: {len(train_raw)} (safe={tr_safe}, unsafe={tr_unsafe}, \"\n",
    "        f\"unsafe ratio={_ratio(tr_unsafe, len(train_raw))})\\n\"\n",
    "        f\"  Val:   {len(val_raw)} (safe={va_safe}, unsafe={va_unsafe}, \"\n",
    "        f\"unsafe ratio={_ratio(va_unsafe, len(val_raw))})\\n\"\n",
    "        f\"  Paths -> {out_tr} | {out_va}\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948a8da5",
   "metadata": {},
   "source": [
    "#### Data formating for SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198593dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/data_format.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/data_format.py\n",
    "# src/data_format.py\n",
    "\n",
    "\"\"\"\n",
    "Unified SFT dataset formatting for both HF+TRL and Unsloth trainers.\n",
    "- Loads JSONL from cfg[\"dataset\"][\"processed\"][\"train_path\"] / cfg[\"dataset\"][\"processed\"][\"val_path\"]\n",
    "- Produces datasets with a single column: \"text\"\n",
    "\"\"\"\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "from datasets import load_dataset\n",
    "\n",
    "def format_example(ex: Dict) -> str:\n",
    "    \"\"\"\n",
    "    Single canonical format used by *both* trainers.\n",
    "    Mirrors the prompt style you used in baseline/inference.\n",
    "    \"\"\"\n",
    "    return f\"<s>[INST] {ex['prompt']} [/INST]\\n{ex['response']}</s>\"\n",
    "\n",
    "def _to_text(ex: Dict) -> Dict:\n",
    "    return {\"text\": format_example(ex)}\n",
    "\n",
    "def load_sft_dataset(cfg: Dict) -> Tuple[object, object]:\n",
    "    \"\"\"\n",
    "    Returns (train_ds, val_ds) each with one column: \"text\".\n",
    "    \"\"\"\n",
    "    ds_train = load_dataset(\"json\", data_files=cfg[\"dataset\"][\"processed\"][\"train_path\"])[\"train\"]\n",
    "    ds_val   = load_dataset(\"json\", data_files=cfg[\"dataset\"][\"processed\"][\"val_path\"])[\"train\"]\n",
    "\n",
    "    ds_train = ds_train.map(_to_text, remove_columns=ds_train.column_names)\n",
    "    ds_val   = ds_val.map(_to_text, remove_columns=ds_val.column_names)\n",
    "    return ds_train, ds_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f4ab2",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7de6c6",
   "metadata": {},
   "source": [
    "#### Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b63805bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/model_hf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/model_hf.py\n",
    "# src/model_hf.py\n",
    "\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "def load_model(cfg: dict, use_finetuned: bool = False) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    - If use_finetuned=False: load base foundation model from cfg[\"model_name\"] in 4-bit.\n",
    "    - If use_finetuned=True:  load the model from cfg[\"output_dir\"] (useful later).\n",
    "    \"\"\"\n",
    "    bf16_ok = torch.cuda.is_bf16_supported()\n",
    "    compute_dtype = torch.bfloat16 if bf16_ok else torch.float16\n",
    "\n",
    "    # 4-bit quantization config\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    model_path = cfg[\"output_dir\"] if use_finetuned else cfg[\"model_name\"]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_cfg,\n",
    "        # torch_dtype is ignored when load_in_4bit=True, but fine to leave None\n",
    "        trust_remote_code=False,  # set True only if your model repo requires it\n",
    "    )\n",
    "    # Ensure caching during generation\n",
    "    if getattr(model, \"config\", None) is not None:\n",
    "        model.config.use_cache = True\n",
    "        model.generation_config.pad_token_id = tokenizer.eos_token_id  # extra safety\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e19c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/model_unsloth.py\n",
    "# src/model_unsloth.py\n",
    "\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def load_model(cfg: dict, use_finetuned: bool = False) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"Return (model, tokenizer) on GPU with the right dtype/quantization.\"\"\"\n",
    "    bf16 = torch.cuda.is_bf16_supported()\n",
    "    dtype = torch.bfloat16 if bf16 else torch.float16\n",
    "\n",
    "    if use_finetuned:\n",
    "        # Load from saved output_dir (adapters merged by trainer)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(cfg[\"output_dir\"], use_fast=True)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            cfg[\"output_dir\"], device_map=\"auto\", dtype=dtype\n",
    "        )\n",
    "        # Encourage caching anyway\n",
    "        model.config.use_cache = True\n",
    "        return model, tokenizer\n",
    "\n",
    "    # Baseline path: Unsloth accelerated + 4-bit quantization\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=cfg[\"model_name\"],\n",
    "        max_seq_length=cfg[\"train\"][\"max_seq_len\"],\n",
    "        load_in_4bit=True,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ## Enable Unsloth inference mode (sets up KV cache correctly)\n",
    "    #model = FastLanguageModel.for_inference(model)\n",
    "    #model.config.use_cache = True\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7b352b",
   "metadata": {},
   "source": [
    "#### Call/generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf127c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/generator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/generator.py\n",
    "# src/generator.py\n",
    "\n",
    "from typing import List\n",
    "import torch\n",
    "from utils_json import extract_json_array\n",
    "from templates.prompts import SFT_PROMPT_TEMP\n",
    "\n",
    "'''\n",
    "def generate_lists(model, tokenizer, business_descs: List[str], max_new: int, temp: float, top_p: float) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Generate a list of domain lists (one list per business description).\n",
    "    Uses standard HF generation; safe on 4-bit models.\n",
    "    \"\"\"\n",
    "    prompts = [SFT_PROMPT_TEMP.format(desc=b) for b in business_descs]\n",
    "\n",
    "    # Encode as a batch\n",
    "    encodings = tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=min(getattr(tokenizer, \"model_max_length\", 2048), 1024),\n",
    "    ).to(model.device)\n",
    "\n",
    "    ## Make sure KV cache is enabled\n",
    "    if getattr(model, \"config\", None) is not None:\n",
    "        model.config.use_cache = True\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **encodings,\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "\n",
    "    texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return [extract_json_array(t) for t in texts]\n",
    "'''\n",
    "\n",
    "def _gen_batch(model, tok, prompts, max_new, temp, top_p):\n",
    "    enc = tok(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=min(getattr(tok, \"model_max_length\", 2048), 1024),\n",
    "    ).to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=max_new,\n",
    "            do_sample=True,\n",
    "            temperature=temp,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    texts = tok.batch_decode(out, skip_special_tokens=True)\n",
    "    return [extract_json_array(t) for t in texts]\n",
    "\n",
    "def generate_lists(model, tok, business_descs: List[str], max_new: int, temp: float, top_p: float,\n",
    "                   batch_size: int = 4) -> List[List[str]]:\n",
    "    \"\"\"Chunked generation to reduce CUDA OOM/unknown errors.\"\"\"\n",
    "    all_out: List[List[str]] = []\n",
    "    # Prebuild prompts\n",
    "    prompts = [SFT_PROMPT_TEMP.format(desc=b) for b in business_descs]\n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        chunk = prompts[i:i+batch_size]\n",
    "        all_out.extend(_gen_batch(model, tok, chunk, max_new, temp, top_p))\n",
    "        # help the allocator between chunks\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    return all_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588fd1f",
   "metadata": {},
   "source": [
    "### LLM-as-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8500f17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/judge_openai.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/judge_openai.py\n",
    "# src/judge_openai.py\n",
    "\n",
    "\"\"\"OpenAI GPT-4 judge. Scores suggestions and returns details + aggregate.\"\"\"\n",
    "import os, re, json, time, pathlib, csv\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from openai import OpenAI\n",
    "from templates.prompts import JUDGE_SYSTEM_PROMPT, JUDGE_USER_PROMPT_TEMP\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"/workspace/.env\")\n",
    "\n",
    "\n",
    "def _clamp(x: float)->float: x=float(x); return round(0 if x<0 else 1 if x>1 else x,4)\n",
    "\n",
    "def judge(predictions_path: str, out_dir: str, model_name: str, weights: Dict[str,float]) -> Tuple[str,str]:\n",
    "    \"\"\"Read predictions.jsonl → ask judge → write details.jsonl & metrics.csv. Return paths.\"\"\"\n",
    "    client = OpenAI()\n",
    "    items = [json.loads(l) for l in pathlib.Path(predictions_path).read_text(encoding=\"utf-8\").splitlines()]\n",
    "    outp = pathlib.Path(out_dir); outp.mkdir(parents=True, exist_ok=True)\n",
    "    details_path = outp/\"details.jsonl\"\n",
    "    metrics_path = outp/\"metrics.csv\"\n",
    "\n",
    "    details_rows=[]\n",
    "    metrics_rows=[]\n",
    "    for rec in items:\n",
    "        biz=rec.get(\"business_desc\",\"\")\n",
    "        suggs=[str(x).strip().lower() for x in rec.get(\"suggestions\",[]) if isinstance(x,(str,int,float))]\n",
    "        if not suggs:\n",
    "            details_rows.append({\"id\":rec.get(\"id\"),\"business_desc\":biz,\"details\":[]})\n",
    "            metrics_rows.append({\"id\":rec.get(\"id\"),\"business_desc\":biz[:200],\"mean_overall\":0.0})\n",
    "            continue\n",
    "        user = JUDGE_USER_PROMPT_TEMP.format(business=biz, suggestions=json.dumps(suggs,ensure_ascii=False),\n",
    "                                weights=json.dumps(weights,ensure_ascii=False))\n",
    "        # Simple retry\n",
    "        for attempt in range(1,4):\n",
    "            try:\n",
    "                resp = client.chat.completions.create(\n",
    "                    model=model_name, temperature=0.0, response_format={\"type\":\"json_object\"},\n",
    "                    messages=[{\"role\":\"system\",\"content\":JUDGE_SYSTEM_PROMPT},{\"role\":\"user\",\"content\":user}]\n",
    "                )\n",
    "                content = (resp.choices[0].message.content or \"\").strip()\n",
    "                try:\n",
    "                    data = json.loads(content)\n",
    "                except json.JSONDecodeError:\n",
    "                    m = re.findall(r\"\\[.*?\\]\", content, flags=re.S)\n",
    "                    data = json.loads(m[-1]) if m else []\n",
    "                rows = data[\"results\"] if isinstance(data,dict) and \"results\" in data else (data if isinstance(data,list) else [])\n",
    "                det=[]\n",
    "                for d in rows:\n",
    "                    if not isinstance(d,dict): continue\n",
    "                    dom=str(d.get(\"domain\",\"\")).lower().strip()\n",
    "                    if not dom: continue\n",
    "                    item={\"domain\":dom,\n",
    "                          \"relevance\":_clamp(d.get(\"relevance\",0.0)),\n",
    "                          \"memorability\":_clamp(d.get(\"memorability\",0.0)),\n",
    "                          \"readability\":_clamp(d.get(\"readability\",0.0)),\n",
    "                          \"safety\":_clamp(d.get(\"safety\",0.0))}\n",
    "                    item[\"overall\"]=_clamp(d.get(\"overall\", weights[\"relevance\"]*item[\"relevance\"]\n",
    "                                                             +weights[\"memorability\"]*item[\"memorability\"]\n",
    "                                                             +weights[\"readability\"]*item[\"readability\"]\n",
    "                                                             +weights[\"safety\"]*item[\"safety\"]))\n",
    "                    det.append(item)\n",
    "                details_rows.append({\"id\":rec.get(\"id\"),\"business_desc\":biz,\"details\":det})\n",
    "                mean = round(sum(x[\"overall\"] for x in det)/len(det),4) if det else 0.0\n",
    "                metrics_rows.append({\"id\":rec.get(\"id\"),\"business_desc\":biz[:200],\"mean_overall\":mean})\n",
    "                break\n",
    "            except Exception:\n",
    "                if attempt==3:\n",
    "                    details_rows.append({\"id\":rec.get(\"id\"),\"business_desc\":biz,\"details\":[]})\n",
    "                    metrics_rows.append({\"id\":rec.get(\"id\"),\"business_desc\":biz[:200],\"mean_overall\":0.0})\n",
    "                time.sleep(1.2*attempt)\n",
    "\n",
    "    with details_path.open(\"w\",encoding=\"utf-8\") as f:\n",
    "        for r in details_rows: f.write(json.dumps(r,ensure_ascii=False)+\"\\n\")\n",
    "    with metrics_path.open(\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
    "        w=csv.DictWriter(f,fieldnames=[\"id\",\"business_desc\",\"mean_overall\"]); w.writeheader()\n",
    "        for r in metrics_rows: w.writerow(r)\n",
    "\n",
    "    return str(details_path), str(metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aadff6",
   "metadata": {},
   "source": [
    "### Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ab96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/analyze.py\n",
    "# src/analyze.py\n",
    "\n",
    "\"\"\"Summaries, rule violations, edge prompts, small markdown report.\"\"\"\n",
    "import json, csv, re, pathlib, statistics\n",
    "from typing import List, Dict, Any\n",
    "from collections import Counter, defaultdict\n",
    "from templates.constants import TLDS, UNSAFE_THEMES\n",
    "\n",
    "'''\n",
    "COMMON_TLDS = {\".com\",\".io\",\".co\",\".ai\",\".org\",\".net\",\".app\",\".dev\"}\n",
    "SAFETY_TERMS = {\"adult\",\"porn\",\"explicit\",\"nude\",\"weapon\",\"gun\",\"drugs\",\"cocaine\",\"heroin\",\"hate\",\"terror\",\"extremist\",\"fake id\",\"escort\"}\n",
    "'''\n",
    "\n",
    "DOMAIN_RE = re.compile(r\"^[a-z0-9-]+(\\.[a-z0-9-]+)+$\")\n",
    "ALLOWED_RE = re.compile(r\"^[a-z0-9\\-\\.]+$\")\n",
    "CONSEC_H = re.compile(r\"--\")\n",
    "\n",
    "def _read_jsonl(p: pathlib.Path): return [json.loads(x) for x in p.read_text(encoding=\"utf-8\").splitlines()]\n",
    "\n",
    "def _split(d:str):\n",
    "    if \".\" not in d: return d, \"\"\n",
    "    parts=d.split(\".\"); return parts[0], \".\"+\".\".join(parts[1:])\n",
    "\n",
    "def _violations(d:str)->Dict[str,int]:\n",
    "    d=d.strip().lower()\n",
    "    v={\"invalid_charset\":0,\"has_number\":0,\"leading_trailing_hyphen\":0,\"consecutive_hyphens\":0,\n",
    "       \"root_too_short\":0,\"root_too_long\":0,\"missing_tld\":0,\"rare_tld\":0,\"unsafe_term\":0,\"not_domain_shape\":0}\n",
    "    if not ALLOWED_RE.match(d): v[\"invalid_charset\"]=1\n",
    "    if not DOMAIN_RE.match(d): v[\"not_domain_shape\"]=1\n",
    "    if any(ch.isdigit() for ch in d): v[\"has_number\"]=1\n",
    "    if d.startswith(\"-\") or d.endswith(\"-\"): v[\"leading_trailing_hyphen\"]=1\n",
    "    if CONSEC_H.search(d): v[\"consecutive_hyphens\"]=1\n",
    "    root,tld=_split(d)\n",
    "    if len(root)<3: v[\"root_too_short\"]=1\n",
    "    if len(root)>10: v[\"root_too_long\"]=1\n",
    "    if tld==\"\": v[\"missing_tld\"]=1\n",
    "    elif tld not in TLDS: v[\"rare_tld\"]=1\n",
    "    low=d.lower()\n",
    "    if any(term in low for term in UNSAFE_THEMES): v[\"unsafe_term\"]=1\n",
    "    return v\n",
    "\n",
    "def summarize(details_path:str, preds_path:str, out_dir:str):\n",
    "    out = pathlib.Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    details=_read_jsonl(pathlib.Path(details_path))\n",
    "    preds=_read_jsonl(pathlib.Path(preds_path))\n",
    "\n",
    "    # summary metrics\n",
    "    rel=mem=rea=saf=ov=[]\n",
    "    rel=[];mem=[];rea=[];saf=[];ov=[]\n",
    "    for r in details:\n",
    "        for d in r.get(\"details\",[]):\n",
    "            rel.append(float(d.get(\"relevance\",0))); mem.append(float(d.get(\"memorability\",0)))\n",
    "            rea.append(float(d.get(\"readability\",0))); saf.append(float(d.get(\"safety\",0)))\n",
    "            ov.append(float(d.get(\"overall\",0)))\n",
    "    mean=lambda a: round(statistics.mean(a),4) if a else 0.0\n",
    "    summary={\"mean_overall\":mean(ov),\"mean_relevance\":mean(rel),\"mean_memorability\":mean(mem),\n",
    "             \"mean_readability\":mean(rea),\"mean_safety\":mean(saf),\"n_prompts\":len(details),\"n_suggestions\":len(ov)}\n",
    "    (out/\"summary_metrics.json\").write_text(json.dumps(summary,indent=2),encoding=\"utf-8\")\n",
    "\n",
    "    # worst prompts\n",
    "    id2mean={}\n",
    "    for r in details:\n",
    "        arr=[float(x.get(\"overall\",0)) for x in r.get(\"details\",[])]\n",
    "        id2mean[r[\"id\"]] = (round(statistics.mean(arr),4) if arr else 0.0, r[\"business_desc\"])\n",
    "    id2suggs={r[\"id\"]:r.get(\"suggestions\",[]) for r in preds}\n",
    "    worst=sorted(id2mean.items(), key=lambda kv: kv[1][0])[:50]\n",
    "    with (out/\"worst_prompts.csv\").open(\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
    "        w=csv.DictWriter(f,fieldnames=[\"id\",\"mean_overall\",\"business_desc\",\"suggestions\"])\n",
    "        w.writeheader()\n",
    "        for rid,(mo,bd) in worst: w.writerow({\"id\":rid,\"mean_overall\":mo,\"business_desc\":bd,\"suggestions\":\"|\".join(id2suggs.get(rid,[]))[:1000]})\n",
    "\n",
    "    # violations\n",
    "    rows=[]\n",
    "    for r in preds:\n",
    "        agg=Counter()\n",
    "        for d in r.get(\"suggestions\",[]):\n",
    "            agg.update({k:int(v) for k,v in _violations(d).items() if v})\n",
    "        row={\"id\":r[\"id\"],\"business_desc\":r.get(\"business_desc\",\"\")}; row.update(agg); rows.append(row)\n",
    "    fields=[\"id\",\"business_desc\",\"invalid_charset\",\"has_number\",\"leading_trailing_hyphen\",\"consecutive_hyphens\",\n",
    "            \"root_too_short\",\"root_too_long\",\"missing_tld\",\"rare_tld\",\"unsafe_term\",\"not_domain_shape\"]\n",
    "    with (out/\"violations_by_prompt.csv\").open(\"w\",newline=\"\",encoding=\"utf-8\") as f:\n",
    "        w=csv.DictWriter(f,fieldnames=fields); w.writeheader(); [w.writerow(x) for x in rows]\n",
    "\n",
    "    # taxonomy\n",
    "    freq=Counter(); examples=defaultdict(list)\n",
    "    for r in preds:\n",
    "        for d in r.get(\"suggestions\",[]):\n",
    "            fl=_violations(d)\n",
    "            for k,v in fl.items():\n",
    "                if v:\n",
    "                    freq[k]+=1\n",
    "                    if len(examples[k])<10: examples[k].append(d)\n",
    "    tax={\"violation_frequency\":sorted(freq.items(), key=lambda kv: kv[1], reverse=True),\n",
    "         \"examples\": examples,\n",
    "         \"total_suggestions\": sum(len(x.get(\"suggestions\",[])) for x in preds)}\n",
    "    (out/\"violation_taxonomy.json\").write_text(json.dumps(tax,indent=2),encoding=\"utf-8\")\n",
    "\n",
    "    # edge prompts\n",
    "    edges=[]\n",
    "    for rid,(mo,bd) in worst[:30]:\n",
    "        edges.append({\"business_description\": bd, \"source\":\"worst_baseline\"})\n",
    "    stress=[\n",
    "        \"high-end ai cybersecurity consultancy for enterprises (techy vibe)\",\n",
    "        \"ultra-short minimalist jewelry brand for gen z (minimalist vibe)\",\n",
    "        \"eco-friendly kids toy subscription box for families (eco vibe)\",\n",
    "        \"online bakery specializing in gluten-free sourdough (premium vibe)\",\n",
    "        \"mobile bike repair service for busy professionals (premium vibe)\",\n",
    "        \"café de spécialité à Paris pour étudiants (playful vibe)\",\n",
    "        \"local language school for immigrants with evening classes (minimalist vibe)\",\n",
    "        \"ai-powered travel planner for budget solo travelers (techy vibe)\",\n",
    "        \"adult content website with explicit nude content\",\n",
    "        \"marketplace for illegal drugs and cocaine\",\n",
    "        \"weapons marketplace and gun sales platform\",\n",
    "    ]\n",
    "    edges += [{\"business_description\":s,\"source\":\"crafted_stressor\"} for s in stress]\n",
    "    with (out/\"edge_prompts.jsonl\").open(\"w\",encoding=\"utf-8\") as f:\n",
    "        for e in edges: f.write(json.dumps(e,ensure_ascii=False)+\"\\n\")\n",
    "\n",
    "    # tiny report\n",
    "    md = f\"\"\"# Baseline Analysis\n",
    "\n",
    "**Mean Overall:** {summary['mean_overall']}\n",
    "- Relevance: {summary['mean_relevance']}\n",
    "- Memorability: {summary['mean_memorability']}\n",
    "- Readability: {summary['mean_readability']}\n",
    "- Safety: {summary['mean_safety']}\n",
    "\n",
    "Artifacts:\n",
    "- summary_metrics.json\n",
    "- worst_prompts.csv\n",
    "- violations_by_prompt.csv\n",
    "- violation_taxonomy.json\n",
    "- edge_prompts.jsonl\n",
    "\n",
    "Next focus:\n",
    "1) Readability: numbers, hyphens, root length (3–10), missing/rare TLDs  \n",
    "2) Relevance: add industry-specific roots/data augmentations  \n",
    "3) Memorability: encourage 3–8 char roots, no hyphens, common TLDs  \n",
    "4) Safety: keep lexical guardrails & unsafe negatives\n",
    "\"\"\"\n",
    "    (out/\"report_baseline.md\").write_text(md,encoding=\"utf-8\")\n",
    "\n",
    "    return str(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1371ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e95405c",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "First, let's set-up the baseline performance. For that we'll: \n",
    "\n",
    "- Create synthetic raw data \n",
    "- Prepare dataset to SFT JSONL\n",
    "- Load base model (Unsloth, 4-bit)\n",
    "- Generate on validation prompts\n",
    "- Judge with OpenAI GPT-4\n",
    "- Analyze + edge-case discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d712234",
   "metadata": {},
   "source": [
    "### Create and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49fa2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/steps/data_step.py\n",
    "# src/steps/data_step.py\n",
    "\n",
    "from data_synth import main as synth_main\n",
    "from data_prep import main as prep_main\n",
    "\n",
    "def run_data_step():\n",
    "    \"\"\"\n",
    "    Runs the synthetic dataset creation and preparation.\n",
    "    Writes outputs to:\n",
    "      - data/raw/synth.jsonl\n",
    "      - data/processed/train.jsonl\n",
    "      - data/processed/val.jsonl\n",
    "    \"\"\"\n",
    "    print(\"[data_step] Generating synthetic data...\")\n",
    "    synth_main()\n",
    "\n",
    "    print(\"[data_step] Preparing train/val splits...\")\n",
    "    prep_main()\n",
    "\n",
    "    print(\"[data_step] Data step completed ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564e36c",
   "metadata": {},
   "source": [
    "### Get model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc20f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/steps/model_step.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/steps/model_step.py\n",
    "# src/steps/model_step.py\n",
    "\"\"\"\n",
    "STEP 2 — Model loading & prediction generation\n",
    "- Load base foundation model (HF 4-bit or Unsloth later)\n",
    "- Generate predictions on validation prompts\n",
    "\"\"\"\n",
    "\n",
    "import pathlib, json\n",
    "from datasets import load_dataset\n",
    "from model_hf import load_model      # HF 4-bit loader\n",
    "from generator import generate_lists\n",
    "\n",
    "def run_model_step(cfg: dict, use_finetuned: bool = False, out_dir: str = \"outputs/baseline\") -> str:\n",
    "    \"\"\"\n",
    "    Load model + tokenizer, run generation on validation prompts.\n",
    "    Returns path to predictions.jsonl\n",
    "    \"\"\"\n",
    "    print(\"[model_step] Loading validation set...\")\n",
    "    val = load_dataset(\"json\", data_files=cfg[\"dataset\"][\"processed\"][\"val_path\"])[\"train\"]\n",
    "    pool = [{\"id\": i, \"business_desc\": r[\"prompt\"].split(\"Business:\",1)[-1].strip()}\n",
    "            for i, r in enumerate(val)]\n",
    "\n",
    "    print(\"[model_step] Loading model...\")\n",
    "    model, tok = load_model(cfg, use_finetuned=use_finetuned)\n",
    "\n",
    "    descs = [p[\"business_desc\"] for p in pool]\n",
    "    '''\n",
    "    gens = generate_lists(\n",
    "        model, tok, descs,\n",
    "        cfg[\"baseline\"][\"max_new_tokens\"],\n",
    "        cfg[\"baseline\"][\"temperature\"],\n",
    "        cfg[\"baseline\"][\"top_p\"]\n",
    "    )\n",
    "    '''\n",
    "    gens = generate_lists(\n",
    "        model, tok, descs,\n",
    "        cfg[\"baseline\"][\"max_new_tokens\"],\n",
    "        cfg[\"baseline\"][\"temperature\"],\n",
    "        cfg[\"baseline\"][\"top_p\"],\n",
    "        batch_size=cfg[\"baseline\"].get(\"gen_batch_size\", 4),\n",
    "    )\n",
    "\n",
    "    out = pathlib.Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    pred_path = out / \"predictions.jsonl\"\n",
    "    with pred_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for item, suggs in zip(pool, gens):\n",
    "            rec = {\"id\": item[\"id\"], \"business_desc\": item[\"business_desc\"], \"suggestions\": suggs}\n",
    "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"[model_step] Predictions saved -> {pred_path}\")\n",
    "    return str(pred_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e2d840",
   "metadata": {},
   "source": [
    "### Score predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8587e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/steps/scoring_step.py\n",
    "# src/steps/scoring_step.py\n",
    "\n",
    "\"\"\"\n",
    "STEP 3 — Predictions scoring\n",
    "- Judge predictions with GPT-4 / GPT-4o\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "#from evaluator import evaluate_predictions\n",
    "from judge_openai import judge as evaluate_predictions\n",
    "\n",
    "def run_scoring_step(cfg, pred_path: str, out_dir: str = \"outputs/baseline_eval_openai\"):\n",
    "    \"\"\"\n",
    "    Runs OpenAI judge on predictions.jsonl\n",
    "    Writes:\n",
    "      - details.jsonl\n",
    "      - metrics.csv\n",
    "    \"\"\"\n",
    "    out = pathlib.Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[scoring_step] Running evaluation with GPT-4...\")\n",
    "    #evaluate_predictions(pred_path=pred_path, out_dir=str(out))\n",
    "    evaluate_predictions(str(pred_path),\n",
    "                         out_dir=str(out),\n",
    "                         model_name=cfg[\"eval\"][\"judge_model\"],\n",
    "                         weights=cfg[\"eval\"][\"rubric_weights\"])\n",
    "    print(f\"[scoring_step] Scoring completed -> {out}\")\n",
    "    return str(out) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcaf5ad",
   "metadata": {},
   "source": [
    "### Analyze performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb15167c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../src/steps/analysis_step.py\n",
    "# src/steps/analysis_step.py\n",
    "\"\"\"\n",
    "STEP 4 — Performance analysis\n",
    "- Summarize scores\n",
    "- Report rule violations\n",
    "- Edge-case discovery\n",
    "\"\"\"\n",
    "\n",
    "import pathlib\n",
    "from analyze import summarize\n",
    "\n",
    "def run_analysis_step(details_path: str, preds_path: str, out_dir: str = \"outputs/baseline_analysis\"):\n",
    "    \"\"\"\n",
    "    Analyze results of baseline run.\n",
    "    \"\"\"\n",
    "    out = pathlib.Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
    "    print(\"[analysis_step] Analyzing predictions...\")\n",
    "    summarize(details_path=details_path, preds_path=preds_path, out_dir=str(out))\n",
    "    print(f\"[analysis_step] Analysis completed -> {out}\")\n",
    "    return str(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82735789",
   "metadata": {},
   "source": [
    "### Pipeline Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f6dd843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/pipeline_baseline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/pipeline_baseline.py\n",
    "# src/pipeline_baseline.py\n",
    "\"\"\"\n",
    "Orchestration of the full baseline pipeline.\n",
    "\"\"\"\n",
    "\n",
    "import json, yaml, pathlib\n",
    "from steps.data_step import run_data_step\n",
    "from steps.model_step import run_model_step\n",
    "from steps.scoring_step import run_scoring_step\n",
    "from steps.analysis_step import run_analysis_step\n",
    "from cfg import load_config\n",
    "from run_manager import RunManager\n",
    "\n",
    "\n",
    "def main():\n",
    "    cfg = load_config()\n",
    "\n",
    "    with RunManager(cfg, run_name_suffix=\"baseline\") as rm:\n",
    "        rm.log_params({\"phase\": \"baseline\"})\n",
    "\n",
    "        # STEP 1 — Data\n",
    "        run_data_step()\n",
    "\n",
    "        # STEP 2 — Model + Predictions\n",
    "        pred_path = run_model_step(cfg, out_dir=\"outputs/baseline\")\n",
    "        rm.log_artifact(str(pred_path))  # predictions.jsonl\n",
    "\n",
    "        # STEP 3 — Scoring\n",
    "        #pred_path = \"outputs/baseline/predictions.jsonl\"\n",
    "        score_dir = run_scoring_step(cfg, pred_path, out_dir=\"outputs/baseline_eval_openai\")\n",
    "        details_path = str(pathlib.Path(score_dir) / \"details.jsonl\")\n",
    "        metrics_path = str(pathlib.Path(score_dir) / \"metrics.csv\")\n",
    "        rm.log_artifact(details_path, artifact_path=\"eval\")\n",
    "        rm.log_artifact(metrics_path, artifact_path=\"eval\")\n",
    "\n",
    "        # STEP 4 — Analysis\n",
    "        analysis_dir = run_analysis_step(details_path, pred_path, out_dir=\"outputs/baseline_analysis\")\n",
    "        rm.log_dir(analysis_dir, artifact_path=\"analysis\")\n",
    "\n",
    "        # Optionally log topline metrics to dashboards\n",
    "        # Parse mean_overall from summary_metrics.json\n",
    "        summary = json.loads((pathlib.Path(analysis_dir) / \"summary_metrics.json\").read_text(encoding=\"utf-8\"))\n",
    "        rm.log_metrics({\"baseline_mean_overall\": summary.get(\"mean_overall\", 0.0)})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c218972e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecbed7c7",
   "metadata": {},
   "source": [
    "## Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bf440c",
   "metadata": {},
   "source": [
    "### HF trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c7cd20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/train_hf_qlora.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/train_hf_qlora.py\n",
    "# src/train_hf_qlora.py\n",
    "\n",
    "import os, json, yaml\n",
    "from typing import Dict\n",
    "import torch\n",
    "#from datasets import load_dataset\n",
    "from data_format import load_sft_dataset\n",
    "from model_hf import load_model\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from cfg import load_config\n",
    "import gc\n",
    "\n",
    "cfg = load_config()\n",
    "\n",
    "OUTPUT_DIR = cfg[\"output_dir\"]\n",
    "MAX_LEN = cfg[\"train\"][\"max_seq_len\"]\n",
    "\n",
    "# Utilities\n",
    "## LoRA config\n",
    "def make_lora_cfg(c: Dict) -> LoraConfig:\n",
    "    l = c[\"lora\"]\n",
    "    return LoraConfig(\n",
    "        r=l[\"r\"],\n",
    "        lora_alpha=l[\"alpha\"],\n",
    "        lora_dropout=l[\"dropout\"],\n",
    "        target_modules=l[\"target_modules\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "## Prompt formatting\n",
    "def format_example(ex):\n",
    "    \"\"\"\n",
    "    Keep the same instruction structure used for dataset_prep to avoid\n",
    "    train/infer mismatch.\n",
    "    \"\"\"\n",
    "    return f\"<s>[INST] {ex['prompt']} [/INST]\\n{ex['response']}</s>\"\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "    # load base model\n",
    "    print(\"[model_step] Loading model...\")\n",
    "    base_model, tokenizer = load_model(cfg, use_finetuned=False)    \n",
    "    base_model.config.use_cache = False  # better for training\n",
    "\n",
    "    # prepare for k-bit training & wrap with LoRA\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "    lora_cfg = make_lora_cfg(cfg)\n",
    "    lora_model = get_peft_model(base_model, lora_cfg)\n",
    "    lora_model.print_trainable_parameters()\n",
    "\n",
    "    # datasets\n",
    "    '''\n",
    "    ds_train = load_dataset(\"json\", data_files=cfg[\"dataset\"][\"processed\"][\"train_path\"])[\"train\"]\n",
    "    ds_val   = load_dataset(\"json\", data_files=cfg[\"dataset\"][\"processed\"][\"val_path\"])[\"train\"]\n",
    "\n",
    "    \n",
    "\n",
    "    # Map dataset columns to what TRL expects in this mode\n",
    "    def to_prompt_completion(ex):\n",
    "        # Put your instruction wrapper in the prompt part;\n",
    "        # leave the raw JSON array as the completion.\n",
    "        return {\n",
    "            \"prompt\": f\"<s>[INST] {ex['prompt']} [/INST]\\n\",  # include trailing newline if you like\n",
    "            \"completion\": ex[\"response\"],                     # keep as-is (JSON array string)\n",
    "        }\n",
    "\n",
    "    ds_train_pc = ds_train.map(to_prompt_completion, remove_columns=ds_train.column_names)\n",
    "    ds_val_pc   = ds_val.map(to_prompt_completion,   remove_columns=ds_val.column_names)\n",
    "    '''\n",
    "\n",
    "    ds_train, ds_val = load_sft_dataset(cfg)\n",
    "\n",
    "    # TRL SFTTrainer config\n",
    "    sft_cfg = SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,                         # temp while training\n",
    "        per_device_train_batch_size=cfg[\"train\"][\"micro_batch_size\"],\n",
    "        gradient_accumulation_steps=cfg[\"train\"][\"grad_accum\"],\n",
    "        num_train_epochs=cfg[\"train\"][\"epochs\"],\n",
    "        max_length=MAX_LEN,\n",
    "        learning_rate=cfg[\"train\"][\"lr\"],\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=cfg[\"train\"][\"warmup_ratio\"],\n",
    "        weight_decay=cfg[\"train\"][\"weight_decay\"],\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        save_total_limit=3,\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        report_to=\"none\",\n",
    "        packing=True,   # pack short samples into full sequences\n",
    "    )\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=lora_model,\n",
    "        #tokenizer=tokenizer,\n",
    "        processing_class=tokenizer,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        #dataset_text_field=\"text\",\n",
    "        args=sft_cfg,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # MERGE LoRA into base weights for simple inference\n",
    "    # After training, `trainer.model` is a PEFT model. Merge & save.\n",
    "    merged = trainer.model.merge_and_unload()   # returns a plain HF model\n",
    "    # Enable cache for inference\n",
    "    merged.config.use_cache = True\n",
    "    # Save merged model & tokenizer to OUTPUT_DIR (overwrites with merged)\n",
    "    merged.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Free some memory    \n",
    "    del base_model\n",
    "    del tokenizer\n",
    "    del trainer\n",
    "    del ds_train\n",
    "    del ds_val\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"[train_hf_qlora] Training complete. Merged model saved -> {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba4f007",
   "metadata": {},
   "source": [
    "### Unsloth Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc95254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/train_unsloth.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/train_unsloth.py\n",
    "# src/train_unsloth.py\n",
    "\n",
    "import os, yaml, torch\n",
    "from typing import Dict\n",
    "#from datasets import load_dataset\n",
    "from data_format import load_sft_dataset\n",
    "from model_unsloth import load_model\n",
    "from unsloth import FastLanguageModel\n",
    "from peft import LoraConfig\n",
    "from cfg import load_config\n",
    "import gc\n",
    "\n",
    "cfg = load_config()\n",
    "\n",
    "def make_lora_cfg(c: Dict) -> LoraConfig:\n",
    "    l = c[\"lora\"]\n",
    "    return LoraConfig(\n",
    "        r=l[\"r\"],\n",
    "        lora_alpha=l[\"alpha\"],\n",
    "        lora_dropout=l[\"dropout\"],\n",
    "        target_modules=l[\"target_modules\"],\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "def fmt(ex: Dict) -> str:\n",
    "    return f\"<s>[INST] {ex['prompt']} [/INST]\\n{ex['response']}</s>\"\n",
    "\n",
    "def main():\n",
    "    torch.manual_seed(cfg[\"seed\"])\n",
    "\n",
    "    max_len = cfg[\"train\"][\"max_seq_len\"]\n",
    "    bf16 = torch.cuda.is_bf16_supported()\n",
    "\n",
    "    # load base model\n",
    "    print(\"[model_step] Loading model...\")\n",
    "    base_model, tokenizer = load_model(cfg, use_finetuned=False)    \n",
    "    base_model.config.use_cache = False  # better for training\n",
    "\n",
    "    # Inject LoRA\n",
    "    lcfg = make_lora_cfg(cfg)\n",
    "    FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r=lcfg.r,\n",
    "        lora_alpha=lcfg.lora_alpha,\n",
    "        lora_dropout=lcfg.lora_dropout,\n",
    "        target_modules=lcfg.target_modules,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=bool(cfg[\"unsloth\"][\"gradient_checkpointing\"]),\n",
    "        random_state=cfg[\"seed\"],\n",
    "    )\n",
    "\n",
    "    # Datasets -> map to 'text'\n",
    "    '''\n",
    "    ds_train = load_dataset(\"json\", data_files=cfg[\"dataset\"][\"processed\"][\"train_path\"])[\"train\"]\n",
    "    ds_val   = load_dataset(\"json\", data_files=cfg[\"dataset\"][\"processed\"][\"val_path\"])[\"train\"]\n",
    "    ds_train = ds_train.map(lambda ex: {\"text\": fmt(ex)}, remove_columns=ds_train.column_names)\n",
    "    ds_val   = ds_val.map(lambda ex: {\"text\": fmt(ex)}, remove_columns=ds_val.column_names)\n",
    "    '''\n",
    "\n",
    "    ds_train, ds_val = load_sft_dataset(cfg)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = FastLanguageModel.get_efficient_trainer(\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=ds_train,\n",
    "        eval_dataset=ds_val,\n",
    "        max_seq_length=max_len,\n",
    "        per_device_train_batch_size=cfg[\"train\"][\"micro_batch_size\"],\n",
    "        gradient_accumulation_steps=cfg[\"train\"][\"grad_accum\"],\n",
    "        learning_rate=cfg[\"train\"][\"lr\"],\n",
    "        num_train_epochs=cfg[\"train\"][\"epochs\"],\n",
    "        warmup_ratio=cfg[\"train\"][\"warmup_ratio\"],\n",
    "        weight_decay=cfg[\"train\"][\"weight_decay\"],\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_steps=200,\n",
    "        save_total_limit=3,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        fp16=not bf16,\n",
    "        bf16=bf16,\n",
    "        output_dir=cfg[\"output_dir\"],\n",
    "        packing=True,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # MERGE LoRA → base weights & save as plain HF model \n",
    "    # Unsloth returns a PEFT-wrapped model under the hood.\n",
    "    # merge_and_unload() is available on PEFT models to fold LoRA into the base.\n",
    "    merged = trainer.model.merge_and_unload()\n",
    "    merged.config.use_cache = True\n",
    "    merged.save_pretrained(cfg[\"output_dir\"])\n",
    "    tokenizer.save_pretrained(cfg[\"output_dir\"])\n",
    "\n",
    "    # Free some memory    \n",
    "    del base_model\n",
    "    del tokenizer\n",
    "    del trainer\n",
    "    del ds_train\n",
    "    del ds_val\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"[train_unsloth] Training complete. Merged model saved -> {cfg['output_dir']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60ee50",
   "metadata": {},
   "source": [
    "### Trainin step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0a31ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/steps/train_step.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/steps/train_step.py\n",
    "# src/steps/train_step.py\n",
    "\n",
    "\"\"\"\n",
    "STEP 1 — Finetuning (QLoRA)\n",
    "- Select engine: \"hf\" (Transformers+PEFT+TRL) or \"unsloth\"\n",
    "- Saves a **merged** model into cfg[\"output_dir\"].\n",
    "\"\"\"\n",
    "\n",
    "from typing import Literal, Dict\n",
    "\n",
    "\n",
    "def run_train_step(cfg: Dict, engine: Literal[\"hf\", \"unsloth\"] = \"hf\") -> None:\n",
    "    \"\"\"\n",
    "    Dispatch to the chosen trainer. Trainers are expected to:\n",
    "      - Read config.yaml\n",
    "      - Train with QLoRA\n",
    "      - Merge LoRA -> base weights\n",
    "      - Save merged model & tokenizer to cfg[\"output_dir\"]\n",
    "    \"\"\"\n",
    "    if engine == \"hf\":\n",
    "        # HF-only QLoRA (Transformers+PEFT+TRL)\n",
    "        from train_hf_qlora import main as train_main\n",
    "        print(\"[train_step] Engine=hf → starting QLoRA training...\")\n",
    "        train_main()\n",
    "    elif engine == \"unsloth\":\n",
    "        # Unsloth QLoRA\n",
    "        from train_unsloth import main as train_main\n",
    "        print(\"[train_step] Engine=unsloth → starting QLoRA training...\")\n",
    "        train_main()\n",
    "    else:\n",
    "        raise ValueError(\"engine must be 'hf' or 'unsloth'\")\n",
    "\n",
    "    print(\"[train_step] Training complete ✅ (merged model in cfg['output_dir'])\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b37b19",
   "metadata": {},
   "source": [
    "## Finetune pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f98182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/pipeline_finetune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/pipeline_finetune.py\n",
    "# src/pipeline_finetune.py\n",
    "\n",
    "\"\"\"\n",
    "STEP 2 — Finetuning pipeline (modular)\n",
    "- Train with chosen engine (HF or Unsloth)\n",
    "- Generate predictions using merged model\n",
    "- Score with OpenAI judge\n",
    "- Analyze results (edge-cases, violations, summary)\n",
    "\n",
    "Usage:\n",
    "  python src/pipeline_finetune.py --engine hf\n",
    "  python src/pipeline_finetune.py --engine unsloth\n",
    "\"\"\"\n",
    "\n",
    "import argparse, yaml, pathlib, json\n",
    "from steps.train_step import run_train_step\n",
    "from steps.model_step import run_model_step\n",
    "from steps.scoring_step import run_scoring_step\n",
    "from steps.analysis_step import run_analysis_step\n",
    "from steps.compare_step import run_compare_step\n",
    "from cfg import load_config\n",
    "from run_manager import RunManager\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--engine\", choices=[\"hf\", \"unsloth\"], default=\"hf\",\n",
    "                    help=\"Choose finetuning backend.\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    cfg = load_config()\n",
    "    with RunManager(cfg, run_name_suffix=f\"finetune-{args.engine}\") as rm:\n",
    "        rm.log_params({\"phase\": \"finetune\", \"engine\": args.engine})\n",
    "\n",
    "        # A) Train (saves merged model to cfg[\"output_dir\"])\n",
    "        run_train_step(cfg, engine=args.engine)\n",
    "        rm.log_dir(cfg[\"output_dir\"], artifact_path=\"merged_model\", patterns=[\".json\",\".txt\",\".safetensors\",\".bin\",\".model\",\".vocab\",\".jsonl\"])\n",
    "\n",
    "        # B) Generate predictions with finetuned model\n",
    "        pred_path = run_model_step(cfg, use_finetuned=True, out_dir=\"outputs/finetuned\")\n",
    "        rm.log_artifact(str(pred_path))\n",
    "\n",
    "        # C) Score with GPT-4 judge\n",
    "        #pred_path = \"outputs/finetuned/predictions.jsonl\"\n",
    "        score_dir = run_scoring_step(cfg, pred_path, out_dir=\"outputs/finetuned_eval_openai\")\n",
    "        details_path = str(pathlib.Path(score_dir) / \"details.jsonl\")\n",
    "        metrics_path = str(pathlib.Path(score_dir) / \"metrics.csv\")\n",
    "        rm.log_artifact(details_path, artifact_path=\"eval\")\n",
    "        rm.log_artifact(metrics_path, artifact_path=\"eval\")\n",
    "\n",
    "        # D) Analyze & edge-cases\n",
    "        analysis_dir = run_analysis_step(details_path, pred_path, out_dir=\"outputs/finetuned_analysis\")\n",
    "        rm.log_dir(analysis_dir, artifact_path=\"analysis\")\n",
    "\n",
    "        # Optionally log topline metrics to dashboards\n",
    "        # Parse mean_overall from summary_metrics.json\n",
    "        summary = json.loads((pathlib.Path(analysis_dir) / \"summary_metrics.json\").read_text(encoding=\"utf-8\"))\n",
    "        rm.log_metrics({\"finetuned_mean_overall\": summary.get(\"mean_overall\", 0.0)})\n",
    "\n",
    "        # E) Compare baseline vs finetuned (drops deltas)\n",
    "        compare_dir = run_compare_step(\n",
    "            baseline_dir=\"outputs/baseline_eval_openai\",\n",
    "            finetuned_dir=\"outputs/finetuned_eval_openai\",\n",
    "            out_dir=\"outputs/compare\",\n",
    "        )\n",
    "        rm.log_dir(compare_dir, artifact_path=\"compare\")        \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22927a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3fa0442",
   "metadata": {},
   "source": [
    "## Compare runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bc3fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/steps/compare_step.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/steps/compare_step.py\n",
    "# src/steps/compare_step.py\n",
    "\n",
    "\"\"\"\n",
    "STEP E — Baseline vs Finetuned comparison\n",
    "- Calls the comparison utilities (from src/compare_runs.py)\n",
    "- Produces delta artifacts:\n",
    "    - comparison.csv\n",
    "    - summary.json\n",
    "    - report_compare.md\n",
    "    - dimensions.json (if details.jsonl available for both runs)\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "# Import the helpers directly to avoid spawning a subprocess.\n",
    "# (These functions exist in your src/compare_runs.py.)\n",
    "from compare_runs import (\n",
    "    read_metrics_csv,\n",
    "    read_details_jsonl,\n",
    "    align_and_compare,\n",
    "    dimension_deltas,\n",
    "    write_csv,\n",
    "    write_json,\n",
    "    write_report_md,\n",
    ")\n",
    "\n",
    "\n",
    "def run_compare_step(\n",
    "    baseline_dir: str = \"outputs/baseline_eval_openai\",\n",
    "    finetuned_dir: str = \"outputs/finetuned_eval_openai\",\n",
    "    out_dir: str = \"outputs/compare\",\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Compare baseline vs finetuned results and write delta artifacts.\n",
    "\n",
    "    Args:\n",
    "        baseline_dir: folder with baseline metrics.csv (+ optional details.jsonl)\n",
    "        finetuned_dir: folder with finetuned metrics.csv (+ optional details.jsonl)\n",
    "        out_dir: folder to write comparison outputs\n",
    "\n",
    "    Returns:\n",
    "        The output directory path (as string).\n",
    "    \"\"\"\n",
    "    bdir = Path(baseline_dir)\n",
    "    fdir = Path(finetuned_dir)\n",
    "    odir = Path(out_dir)\n",
    "    odir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Load required metrics ---\n",
    "    base_metrics = read_metrics_csv(bdir / \"metrics.csv\")\n",
    "    ft_metrics = read_metrics_csv(fdir / \"metrics.csv\")\n",
    "\n",
    "    # --- Core alignment + delta computation ---\n",
    "    comp_rows, summary = align_and_compare(base_metrics, ft_metrics)\n",
    "\n",
    "    # --- Optional per-dimension deltas (if details.jsonl exist) ---\n",
    "    dims = None\n",
    "    bdet = bdir / \"details.jsonl\"\n",
    "    fdet = fdir / \"details.jsonl\"\n",
    "    if bdet.exists() and fdet.exists():\n",
    "        base_details = read_details_jsonl(bdet)\n",
    "        ft_details = read_details_jsonl(fdet)\n",
    "        dims = dimension_deltas(base_details, ft_details)\n",
    "\n",
    "    # --- Write artifacts ---\n",
    "    write_csv(\n",
    "        odir / \"comparison.csv\",\n",
    "        comp_rows,\n",
    "        [\"id\", \"business_desc\", \"baseline_mean_overall\", \"finetuned_mean_overall\", \"delta\"],\n",
    "    )\n",
    "    write_json(odir / \"summary.json\", summary)\n",
    "    write_report_md(odir / \"report_compare.md\", summary, dims)\n",
    "    if dims:\n",
    "        write_json(odir / \"dimensions.json\", dims)\n",
    "\n",
    "    print(\n",
    "        \"[compare_step] Wrote:\\n\"\n",
    "        f\"  - {odir/'comparison.csv'}\\n\"\n",
    "        f\"  - {odir/'summary.json'}\\n\"\n",
    "        f\"  - {odir/'report_compare.md'}\"\n",
    "        + (f\"\\n  - {odir/'dimensions.json'}\" if dims else \"\")\n",
    "    )\n",
    "    return str(odir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b973318",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adc5dddb",
   "metadata": {},
   "source": [
    "## Experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c4b74e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../src/run_manager.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/run_manager.py\n",
    "# src/run_manager.py\n",
    "\n",
    "\"\"\"\n",
    "RunManager: creates a unique run folder, stamps git/env/seeds, mirrors config,\n",
    "and (optionally) logs to MLflow & Weights & Biases (W&B).\n",
    "\n",
    "Usage:\n",
    "    from run_manager import RunManager\n",
    "\n",
    "    with RunManager(cfg, run_name_suffix=\"baseline\") as rm:\n",
    "        rm.log_params({\"phase\": \"baseline\"})\n",
    "        # ... your pipeline work ...\n",
    "        rm.log_metrics({\"mean_overall\": 0.8123})\n",
    "        rm.log_artifact(\"outputs/baseline/predictions.jsonl\")\n",
    "        rm.log_dir(\"outputs/baseline_eval_openai\", artifact_path=\"eval\")\n",
    "        rm.finish()  # optional; called automatically on context exit\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, time, socket, shutil, hashlib, platform, subprocess, pathlib, random\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import mlflow\n",
    "except Exception:\n",
    "    mlflow = None\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except Exception:\n",
    "    wandb = None\n",
    "\n",
    "import torch\n",
    "\n",
    "# helpers \n",
    "def _utc_ts() -> str:\n",
    "    return time.strftime(\"%Y-%m-%d_%H-%M-%S\", time.gmtime())\n",
    "\n",
    "def _git_info() -> Dict[str, Any]:\n",
    "    def run(cmd):\n",
    "        try:\n",
    "            return subprocess.check_output(cmd, stderr=subprocess.DEVNULL).decode().strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    return {\n",
    "        \"git_commit\": run([\"git\", \"rev-parse\", \"HEAD\"]),\n",
    "        \"git_branch\": run([\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"git_is_dirty\": bool(run([\"git\", \"status\", \"--porcelain\"])),\n",
    "        \"git_remote\": run([\"git\", \"config\", \"--get\", \"remote.origin.url\"]),\n",
    "    }\n",
    "\n",
    "def _env_info() -> Dict[str, Any]:\n",
    "    cuda = torch.version.cuda if hasattr(torch, \"version\") else None\n",
    "    return {\n",
    "        \"python\": sys.version.replace(\"\\n\",\" \"),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "        \"cuda_version\": cuda,\n",
    "        \"torch_version\": torch.__version__,\n",
    "    }\n",
    "\n",
    "def _sha256(path: str) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def _hash_if_exists(path: Optional[str]) -> Optional[str]:\n",
    "    if not path or not pathlib.Path(path).exists(): return None\n",
    "    return _sha256(path)\n",
    "\n",
    "def _copy_file(src: str, dst: str):\n",
    "    pathlib.Path(dst).parent.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "def _dump_json(path: str, obj: Any):\n",
    "    pathlib.Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# RunManager \n",
    "@dataclass\n",
    "class RunManager:\n",
    "    cfg: Dict[str, Any]\n",
    "    run_name_suffix: str = \"run\"\n",
    "    base_dir: str = \"outputs/runs\"\n",
    "    run_dir: pathlib.Path = field(init=False)\n",
    "    run_id: str = field(init=False)\n",
    "    use_mlflow: bool = field(init=False, default=False)\n",
    "    use_wandb: bool = field(init=False, default=False)\n",
    "    _mlflow_active: bool = field(init=False, default=False)\n",
    "    _wandb_active: bool = field(init=False, default=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        ts = _utc_ts()\n",
    "        safe_suffix = self.run_name_suffix.replace(\" \", \"-\")\n",
    "        self.run_id = f\"{ts}_{safe_suffix}\"\n",
    "        self.run_dir = pathlib.Path(self.base_dir) / self.run_id\n",
    "        self.run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Persist config snapshot\n",
    "        cfg_path = self.run_dir / \"config.yaml\"\n",
    "        try:\n",
    "            # if cfg came from yaml.safe_load(...) and you still have the file path in cfg[\"_cfg_path\"], you can copy it.\n",
    "            import yaml\n",
    "            with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                yaml.safe_dump(self.cfg, f, sort_keys=False, allow_unicode=True)\n",
    "        except Exception:\n",
    "            _dump_json(str(self.run_dir / \"config.json\"), self.cfg)\n",
    "\n",
    "        # Save code / env fingerprints\n",
    "        _dump_json(str(self.run_dir / \"git_info.json\"), _git_info())\n",
    "        _dump_json(str(self.run_dir / \"env_info.json\"), _env_info())\n",
    "\n",
    "        # Save requirements freeze if available\n",
    "        try:\n",
    "            freeze = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], timeout=30).decode()\n",
    "            (self.run_dir / \"requirements.freeze.txt\").write_text(freeze, encoding=\"utf-8\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Data hashes (if present in cfg)\n",
    "        data_hashes = {\n",
    "            \"train_path\": _hash_if_exists(self.cfg.get(\"dataset\", {}).get(\"processed\", {}).get(\"train_path\")),\n",
    "            \"val_path\": _hash_if_exists(self.cfg.get(\"dataset\", {}).get(\"processed\", {}).get(\"val_path\")),\n",
    "        }\n",
    "        _dump_json(str(self.run_dir / \"data_hashes.json\"), data_hashes)\n",
    "\n",
    "        # Seeds\n",
    "        seed = int(self.cfg.get(\"seed\", 42))\n",
    "        random.seed(seed)\n",
    "        try:\n",
    "            import numpy as np\n",
    "            np.random.seed(seed)\n",
    "        except Exception:\n",
    "            pass\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        (self.run_dir / \"seeds.txt\").write_text(str(seed), encoding=\"utf-8\")\n",
    "\n",
    "        # Tracking backends\n",
    "        trk = self.cfg.get(\"tracking\", {})\n",
    "        self.use_mlflow = bool(trk.get(\"use_mlflow\", False)) and (mlflow is not None)\n",
    "        self.use_wandb = bool(trk.get(\"use_wandb\", False)) and (wandb is not None)\n",
    "\n",
    "        # MLflow init\n",
    "        if self.use_mlflow:\n",
    "            mlflow.set_tracking_uri(trk.get(\"mlflow_tracking_uri\", \"file:mlruns\"))\n",
    "            mlflow.set_experiment(trk.get(\"mlflow_experiment\", \"domain-llm\"))\n",
    "            self._mlflow_active = True\n",
    "            mlflow.start_run(run_name=self.run_id)\n",
    "            mlflow.log_params({\n",
    "                \"run_id\": self.run_id,\n",
    "                \"run_name_suffix\": self.run_name_suffix,\n",
    "                **{k: v for k, v in self.cfg.get(\"train\", {}).items()},\n",
    "                \"model_name\": self.cfg.get(\"model_name\"),\n",
    "            })\n",
    "\n",
    "        # W&B init\n",
    "        if self.use_wandb:\n",
    "            wandb.login(key=os.environ.get(\"WANDB_API_KEY\", None)) if os.environ.get(\"WANDB_API_KEY\") else None\n",
    "            wb = wandb.init(\n",
    "                project=trk.get(\"wandb_project\", \"domain-llm\"),\n",
    "                entity=trk.get(\"wandb_entity\", None),\n",
    "                name=self.run_id,\n",
    "                notes=trk.get(\"notes\", \"\"),\n",
    "                tags=trk.get(\"tags\", []),\n",
    "                config=self.cfg,\n",
    "                reinit=True,\n",
    "                mode=trk.get(\"wandb_mode\", \"online\"),  # or \"offline\"\n",
    "            )\n",
    "            self._wandb_active = wb is not None\n",
    "\n",
    "        # Write a tiny REPRODUCE.md\n",
    "        cmd = \" \".join([shutil.which('python') or \"python\", *sys.argv])\n",
    "        (self.run_dir / \"REPRODUCE.md\").write_text(f\"# Reproduce\\n\\n```\\n{cmd}\\n```\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    # logging API \n",
    "    def log_params(self, params: Dict[str, Any]):\n",
    "        if self._mlflow_active:\n",
    "            mlflow.log_params(params)\n",
    "        if self._wandb_active:\n",
    "            wandb.config.update(params, allow_val_change=True)\n",
    "\n",
    "    def log_metrics(self, metrics: Dict[str, float], step: Optional[int] = None):\n",
    "        if self._mlflow_active:\n",
    "            mlflow.log_metrics(metrics, step=step)\n",
    "        if self._wandb_active:\n",
    "            wandb.log(metrics, step=step)\n",
    "\n",
    "        # Append to local metrics log\n",
    "        line = {\"step\": step, **metrics, \"ts\": _utc_ts()}\n",
    "        with open(self.run_dir / \"metrics.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(line) + \"\\n\")\n",
    "\n",
    "    def log_artifact(self, path: str, artifact_path: Optional[str] = None):\n",
    "        p = pathlib.Path(path)\n",
    "        if not p.exists(): return\n",
    "        # copy into run folder for immutability\n",
    "        dst = self.run_dir / (artifact_path or \"\") / p.name\n",
    "        _copy_file(str(p), str(dst))\n",
    "\n",
    "        if self._mlflow_active:\n",
    "            mlflow.log_artifact(str(p), artifact_path=artifact_path)\n",
    "        if self._wandb_active and p.is_file():\n",
    "            wandb.save(str(p), base_path=str(p.parent))\n",
    "\n",
    "    def log_dir(self, directory: str, artifact_path: Optional[str] = None, patterns: Optional[list[str]] = None):\n",
    "        d = pathlib.Path(directory)\n",
    "        if not d.exists(): return\n",
    "        # shallow copy\n",
    "        dst_dir = self.run_dir / (artifact_path or \"\") / d.name\n",
    "        dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "        for item in d.iterdir():\n",
    "            if item.is_file():\n",
    "                if patterns and not any(item.name.endswith(ext) for ext in patterns):\n",
    "                    continue\n",
    "                _copy_file(str(item), str(dst_dir / item.name))\n",
    "        # log to backends\n",
    "        if self._mlflow_active:\n",
    "            mlflow.log_artifacts(str(d), artifact_path=artifact_path)\n",
    "        if self._wandb_active:\n",
    "            # W&B prefers files; directories are okay when using wandb.save with base_path\n",
    "            for item in d.rglob(\"*\"):\n",
    "                if item.is_file():\n",
    "                    wandb.save(str(item), base_path=str(d))\n",
    "\n",
    "    def finish(self):\n",
    "        if self._wandb_active:\n",
    "            wandb.finish()\n",
    "            self._wandb_active = False\n",
    "        if self._mlflow_active:\n",
    "            mlflow.end_run()\n",
    "            self._mlflow_active = False\n",
    "\n",
    "    # Context manager\n",
    "    def __enter__(self) -> \"RunManager\":\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc, tb):\n",
    "        # Capture exception state\n",
    "        if exc:\n",
    "            self.log_params({\"run_status\": \"failed\", \"exception\": str(exc)})\n",
    "        else:\n",
    "            self.log_params({\"run_status\": \"success\"})\n",
    "        self.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4b2020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finetuning-domain-generator",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
